I need to create project with this criteria 
Introduction
This assignment aims to deepen your understanding of Neighborhood-Based Collaborative Filtering, Clustering-Based Collaborative Filtering, and the computational considerations required to scale recommender systems to large datasets. Throughout the assignment, you will work with a real dataset of at least 100,000 users, 1,000 products, and 1 million ratings, applying a complete Recommender Systems pipeline ranging from statistical dataset analysis to clustering and computational complexity evaluation.
This assignment consists of the following THREE Sections.
Section 1: Statistical Analysis
Section 2: Neighborhood CF Filters
Part 1: user-based CF.
Part 2: item-based CF.
Section 3: Clustering-based Collaborative Filters
Part 1: K-means Clustering based on average number of user ratings.
Part 2: K-means Clustering based on average number of common ratings.
Part 3: K-means Clustering based on average number of raters.
Part 4: K-means Clustering for Cold-Start.

So we need to move step by step 
first for part 1 I need to create notebook for project and here's the full dataset gh repo clone zygmuntz/goodbooks-10k
so read and back
Ensure all files are accessible and can be opened in Visual Studio Cod
without modifications
•⁠  ⁠Organized folder structure:
/dataset /section1_statistical_analysis /section2_neighborhood_cf
/part1_user _based_cf
/part2_item_based_cf
section3_clustering_based_cf /part1_user_clustering_avg_ratings /part2_user_clustering_common_ratings /part3_item_clustering_avg_raters
/part4_cold_start_clustering
/utils (helper functions)
/results (output files, visualizations)
README.md
requirements.txt
•⁠  ⁠Code requirements:
Clear comments explaining each major step; Function docstrings; Modular design (separate files for each part); Reproducible results; All code must be runnable in Visual Studio Code without modifications; Avoid absolute file paths (use relative paths only); No proprietary or platform-specific code.
•⁠  ⁠All file paths in your code must be relative to the code file location. Do not use absolute paths (e.g., C:/Users/..) to ensure your code runs correctly when evaluated on a different machine.

so I need now to clone dataset then to do first task only take care to be full read with instructions 

here's task 1 needs

 Assignment Description and requirements
Round your values to 2 decimal places in all calculations.
 Section ONE: Statistical Analysis:
1. Prepare a dataset of at least 100,000 users, ≥ 1000 products, and ≥ 1 million ratings.
2- Preprocess the dataset to adjust the rating on a 1-to-5 scale.
3- Calculate the number of ratings for each user (nu) and save it
4- Calculate the number of ratings for each item (n;) and save it.
5- Compute the average ratings per user (ru) in your dataset and save it.
6- Compute the average ratings per item (ri) in your datasct and save it.
7- Ascendingly order the total number of ratings per item and plot the distribution
8. Compute the number of products based on their average ratings such that G1 is a group of products whose average ratings per product ≤ 1%, 1%<G2≤5%, 5%<G3≤10%, 10%<G4≤20%, 20%<G5≤30%, 30%<G6≤40%, 40%<G7≤50%, 50<G8≤60%, 60%<G9≤70%, 70<G10≤100%.
9. Compute the total number of ratings in each group and order them ascendingly.
10. Plot the distribution of the number of ratings in each group before and after ordering.
11. Select three target users:
• U1 with ≤ 2% ratings
• U2 with ratings >2% and ≤5%
• U3 with ratings ≥5% and ≤10%
12. Select two target items:
• Select the two lowest rated items (11 and I2) as target items
13. Count the number of co-rating users between each target user and other users (No_common_users), and the number of co-rated items between each target item and other items (No_coRated_items).
14. Determine the threshold ß: maximum number of users who have co-rated at least
30% of items with each target user.
15. Save all intermediate results for use in later parts.
16. Compare the results from point 13 & 14 and give your insights into the dataset by evaluating and discussing the matrix sparsity, rating bias and long-tail problems.
Give your comments in a separate section in your report.