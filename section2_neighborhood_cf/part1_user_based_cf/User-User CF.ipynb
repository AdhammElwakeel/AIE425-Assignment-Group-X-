{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13994587,"sourceType":"datasetVersion","datasetId":8918672}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**USER-USER CF**\n\nZeyad Mohamed Fayed \n222102242","metadata":{"id":"-sfqb9Zf8mqr"}},{"cell_type":"markdown","source":"**Case Study 1**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# ----------------------------\n# 1. Load dataset (Kaggle)\n# ----------------------------\nDATASET_PATH = \"/kaggle/input/digital-music-csv/Digital_Music.csv\"\n\nratings = pd.read_csv(\n    DATASET_PATH,\n    header=None,\n    names=['user_id', 'product_id', 'rating', 'timestamp']\n)\n\nprint(\"âœ… Original dataset shape:\", ratings.shape)\n\n# ----------------------------\n# 2. Take sample for faster processing\n# ----------------------------\nratings_sample = ratings.sample(n=50000, random_state=42)\nprint(\"âœ… Sampled dataset shape:\", ratings_sample.shape)\n\n# ----------------------------\n# 3. Filter users/products with very few ratings\n# ----------------------------\nmin_user_ratings = 5\nmin_item_ratings = 5\n\nuser_counts = ratings_sample['user_id'].value_counts()\nitem_counts = ratings_sample['product_id'].value_counts()\n\nratings_filtered = ratings_sample[\n    (ratings_sample['user_id'].isin(user_counts[user_counts >= min_user_ratings].index)) &\n    (ratings_sample['product_id'].isin(item_counts[item_counts >= min_item_ratings].index))\n]\n\nprint(\"âœ… Filtered dataset shape:\", ratings_filtered.shape)\n\n# ----------------------------\n# 4. Create User-Item Matrix\n# ----------------------------\nuser_item_matrix = ratings_filtered.pivot_table(\n    index='user_id',\n    columns='product_id',\n    values='rating',\n    aggfunc='mean'\n).fillna(0)\n\nprint(\"âœ… User-Item Matrix shape:\", user_item_matrix.shape)\n\n# ----------------------------\n# 5. Drop users/items with all zeros (just in case)\n# ----------------------------\nuser_item_matrix = user_item_matrix.loc[(user_item_matrix.sum(axis=1) > 0), \n                                        (user_item_matrix.sum(axis=0) > 0)]\nprint(\"âœ… Cleaned User-Item Matrix shape:\", user_item_matrix.shape)\n\n# ----------------------------\n# 6. Select target user (â‰¥5 ratings)\n# ----------------------------\n# ----------------------------\n# 6. Select target user (â‰¥3 ratings) âœ…\n# ----------------------------\ntarget_user_candidates = user_item_matrix[user_item_matrix.gt(0).sum(axis=1) >= 2].index\n\nif len(target_user_candidates) == 0:\n    raise ValueError(\"No target user found with >=3 ratings. Try lowering the threshold or increase sample size.\")\n\ntarget_user = target_user_candidates[0]\nprint(f\"âœ… Target user: {target_user} (rated {user_item_matrix.loc[target_user].gt(0).sum()} products)\")\n\n\n# 7. Filter users with overlap âœ…\ntarget_ratings = user_item_matrix.loc[target_user]\n\n# Ø®Ù„ÙŠÙ‡ ÙŠØ¬ÙŠØ¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ØªÙ‚ÙŠÙŠÙ… ÙˆØ§Ø­Ø¯ Ù…Ø´ØªØ±Ùƒ Ù…Ø¹ target user\noverlap_users_mask = user_item_matrix.apply(\n    lambda row: ((row > 0) & (target_ratings > 0)).any(),\n    axis=1\n)\n\nsubset_users = user_item_matrix[overlap_users_mask]\n\nprint(f\"âœ… Users with overlap: {subset_users.shape[0]}\")\nprint(f\"âœ… Subset User-Item Matrix shape: {subset_users.shape}\")\n\n\n# ----------------------------\n# 8. Compute cosine similarity (filtered)\n# ----------------------------\nsubset_similarity = cosine_similarity(subset_users, target_ratings.values.reshape(1, -1))\nsubset_similarity_df = pd.DataFrame(\n    subset_similarity.flatten(),\n    index=subset_users.index,\n    columns=['similarity']\n)\n\n# Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù€ target user Ù†ÙØ³Ù‡\nsubset_similarity_df = subset_similarity_df.drop(index=target_user, errors='ignore')\n\n# ----------------------------\n# 9. Get Top 20% similar users\n# ----------------------------\ntop_20_pct = max(int(len(subset_similarity_df) * 0.2), 1)\ntop_similar_users = subset_similarity_df.sort_values(by='similarity', ascending=False).head(top_20_pct)\n\nprint(f\"\\nâœ… Top 20% Similar Users to User {target_user}:\")\nprint(top_similar_users)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:27:39.601881Z","iopub.execute_input":"2025-12-04T20:27:39.602333Z","iopub.status.idle":"2025-12-04T20:27:41.243675Z","shell.execute_reply.started":"2025-12-04T20:27:39.602308Z","shell.execute_reply":"2025-12-04T20:27:41.243033Z"}},"outputs":[{"name":"stdout","text":"âœ… Original dataset shape: (1584082, 4)\nâœ… Sampled dataset shape: (50000, 4)\nâœ… Filtered dataset shape: (105, 4)\nâœ… User-Item Matrix shape: (94, 64)\nâœ… Cleaned User-Item Matrix shape: (94, 64)\nâœ… Target user: B0012298LC (rated 2 products)\nâœ… Users with overlap: 2\nâœ… Subset User-Item Matrix shape: (2, 64)\n\nâœ… Top 20% Similar Users to User B0012298LC:\n            similarity\nuser_id               \nB00136LQC6        0.98\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# ----------------------------\n# 1. Load dataset\n# ----------------------------\nDATASET_PATH = \"/kaggle/input/digital-music-csv/Digital_Music.csv\"\nratings = pd.read_csv(\n    DATASET_PATH,\n    header=None,\n    names=['user_id', 'product_id', 'rating', 'timestamp']\n)\n\nprint(f\"âœ… Original dataset shape: {ratings.shape}\")\n\n# ----------------------------\n# 2. Take a larger sample\n# ----------------------------\nsample_size = 1000000  # Ù…Ù…ÙƒÙ† ØªØ²ÙˆØ¯ Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ RAM\nratings_sample = ratings.sample(n=sample_size, random_state=42)\nprint(f\"âœ… Sampled dataset shape: {ratings_sample.shape}\")\n\n# ----------------------------\n# 3. Filter users/products with very few ratings\n# ----------------------------\nmin_user_ratings = 5\nmin_item_ratings = 5\n\nuser_counts = ratings_sample['user_id'].value_counts()\nitem_counts = ratings_sample['product_id'].value_counts()\n\nratings_filtered = ratings_sample[\n    (ratings_sample['user_id'].isin(user_counts[user_counts >= min_user_ratings].index)) &\n    (ratings_sample['product_id'].isin(item_counts[item_counts >= min_item_ratings].index))\n]\n\nprint(f\"âœ… Filtered dataset shape: {ratings_filtered.shape}\")\n\n# ----------------------------\n# 4. Create User-Item Matrix\n# ----------------------------\nuser_item_matrix = ratings_filtered.pivot_table(\n    index='user_id',\n    columns='product_id',\n    values='rating',\n    aggfunc='mean'\n).fillna(0)\n\n# Drop all-zero users/items just in case\nuser_item_matrix = user_item_matrix.loc[(user_item_matrix.sum(axis=1) > 0),\n                                        (user_item_matrix.sum(axis=0) > 0)]\n\nprint(f\"âœ… User-Item Matrix shape: {user_item_matrix.shape}\")\n\n# ----------------------------\n# 5. Select target user (â‰¥5 ratings)\n# ----------------------------\ntarget_user_candidates = user_item_matrix[user_item_matrix.gt(0).sum(axis=1) >= 5].index\nif len(target_user_candidates) == 0:\n    raise ValueError(\"No target user found with >=5 ratings. Try increasing sample size.\")\n\ntarget_user = target_user_candidates[0]\nprint(f\"âœ… Target user: {target_user} (rated {user_item_matrix.loc[target_user].gt(0).sum()} products)\")\n\n# ----------------------------\n# 6. Filter users with any overlap (â‰¥1 product in common)\n# ----------------------------\nrated_products = user_item_matrix.loc[target_user][user_item_matrix.loc[target_user] > 0].index\nsubset_users = user_item_matrix[user_item_matrix[rated_products].sum(axis=1) >= 1]\nsubset_users = subset_users.drop(index=target_user, errors='ignore')\n\nprint(f\"âœ… Users with overlap: {subset_users.shape[0]}\")\n\n# ----------------------------\n# 7. Compute cosine similarity\n# ----------------------------\nsubset_similarity = cosine_similarity(subset_users, user_item_matrix.loc[target_user].values.reshape(1, -1))\nsubset_similarity_df = pd.DataFrame(\n    subset_similarity.flatten(),\n    index=subset_users.index,\n    columns=['similarity']\n)\n\n# ----------------------------\n# 8. Select Top 20% similar users\n# ----------------------------\ntop_20_pct = max(int(len(subset_similarity_df) * 0.4), 1)\ntop_similar_users = subset_similarity_df.sort_values(by='similarity', ascending=False).head(top_20_pct)\nprint(f\"\\nâœ… Top 20% Similar Users:\\n{top_similar_users}\")\n\n# ----------------------------\n# 9. Predict unknown ratings\n# ----------------------------\npredictions = {}\nglobal_means = user_item_matrix.mean(axis=0)  # fallback\n\nfor product in user_item_matrix.columns:\n    if user_item_matrix.loc[target_user, product] == 0:  # unrated\n        # ratings from top similar users\n        ratings_by_similar = user_item_matrix.loc[top_similar_users.index, product]\n        weights = top_similar_users['similarity']\n        mask = ratings_by_similar > 0\n        if mask.sum() > 0:\n            # weighted average\n            weighted_avg = np.dot(ratings_by_similar[mask], weights[mask]) / weights[mask].sum()\n            predictions[product] = weighted_avg\n        else:\n            # fallback to global average\n            predictions[product] = global_means[product]\n\n# Convert to DataFrame\npredicted_ratings_df = pd.DataFrame.from_dict(predictions, orient='index', columns=['predicted_rating'])\npredicted_ratings_df = predicted_ratings_df.sort_values(by='predicted_rating', ascending=False)\n\nprint(f\"\\nâœ… Predicted ratings for unrated products by {target_user}:\")\nprint(predicted_ratings_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:27:41.244513Z","iopub.execute_input":"2025-12-04T20:27:41.244774Z","iopub.status.idle":"2025-12-04T20:28:15.903167Z","shell.execute_reply.started":"2025-12-04T20:27:41.244756Z","shell.execute_reply":"2025-12-04T20:28:15.902423Z"}},"outputs":[{"name":"stdout","text":"âœ… Original dataset shape: (1584082, 4)\nâœ… Sampled dataset shape: (1000000, 4)\nâœ… Filtered dataset shape: (140283, 4)\nâœ… User-Item Matrix shape: (25938, 22740)\nâœ… Target user: 5557585400 (rated 6 products)\nâœ… Users with overlap: 19\n\nâœ… Top 20% Similar Users:\n            similarity\nuser_id               \nB007J7MQ54        0.41\nB0080OK7YC        0.27\nB008ZVVHXU        0.22\nB00BCEQZMU        0.20\nB001I8YKMC        0.20\nB0018DS15O        0.19\nB003ALAPLU        0.19\n\nâœ… Predicted ratings for unrated products by 5557585400:\n                predicted_rating\nAZWU085F8CUW2               5.00\nA3GHWUGB9FG2PW              5.00\nAJMYOAWK2XKS0               5.00\nA1RWKI4KCX3K72              5.00\nA2OGNFT0OGTBTH              5.00\nA1UZTY06QS5X1Q              5.00\nA22COT2LBTGICF              5.00\nATLZNVLYKP9AZ               5.00\nA1A1W8YFZHVHC3              5.00\nA3KXTPV7NIUUKK              5.00\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# ----------------------------\n# 1. Target user & top users\n# ----------------------------\ntarget_ratings = user_item_matrix.loc[target_user]\nrated_products = target_ratings[target_ratings > 0].index\n\n# Subset of users who rated at least 1 product in common\nsubset_users = user_item_matrix[user_item_matrix[rated_products].sum(axis=1) >= 1]\nsubset_users = subset_users.drop(index=target_user, errors='ignore')\n\n# ----------------------------\n# 2. Compute raw cosine similarity\n# ----------------------------\nraw_similarity = cosine_similarity(subset_users, target_ratings.values.reshape(1, -1)).flatten()\nsimilarity_df = pd.DataFrame({\n    'similarity': raw_similarity,\n    'common_products': (subset_users[rated_products] > 0).sum(axis=1)\n}, index=subset_users.index)\n\n# ----------------------------\n# 3. Compute Discount Factor (DF)\n# ----------------------------\nsimilarity_df['DF'] = similarity_df['common_products'] / len(rated_products)\n\n# ----------------------------\n# 4. Apply threshold áº >= 0.3\n# ----------------------------\nsimilarity_df = similarity_df[similarity_df['DF'] >= 0.1]\n\n# ----------------------------\n# 5. Compute Discounted Similarity (DS)\n# ----------------------------\nsimilarity_df['DS'] = similarity_df['similarity'] * similarity_df['DF']\n\n# ----------------------------\n# 6. Sort by DS\n# ----------------------------\nsimilarity_df = similarity_df.sort_values(by='DS', ascending=False)\n\nprint(f\"\\nâœ… Discounted Similarities for User {target_user}:\")\nprint(similarity_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:15.904115Z","iopub.execute_input":"2025-12-04T20:28:15.904401Z","iopub.status.idle":"2025-12-04T20:28:15.932891Z","shell.execute_reply.started":"2025-12-04T20:28:15.904372Z","shell.execute_reply":"2025-12-04T20:28:15.932220Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Discounted Similarities for User 5557585400:\n            similarity  common_products   DF   DS\nuser_id                                          \nB007J7MQ54        0.41                1 0.17 0.07\nB0080OK7YC        0.27                1 0.17 0.04\nB008ZVVHXU        0.22                1 0.17 0.04\nB00BCEQZMU        0.20                1 0.17 0.03\nB001I8YKMC        0.20                1 0.17 0.03\nB0018DS15O        0.19                1 0.17 0.03\nB003ALAPLU        0.19                1 0.17 0.03\nB005G9D5G6        0.17                1 0.17 0.03\nB000TER7I0        0.17                1 0.17 0.03\nB0013JWOZG        0.16                1 0.17 0.03\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ----------------------------\n# 1. Compute Top 20% users based on DS\n# ----------------------------\ntop_20_pct_count = max(int(len(similarity_df) * 0.2), 1)  # ØªØ£ÙƒØ¯ Ø¥Ù† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ÙˆØ§Ø­Ø¯\ntop_users_DS = similarity_df.sort_values(by='DS', ascending=False).head(top_20_pct_count)\n\nprint(f\"\\nâœ… Top 20% Most Similar Users to {target_user} (using DS):\")\nprint(top_users_DS)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:15.933715Z","iopub.execute_input":"2025-12-04T20:28:15.933948Z","iopub.status.idle":"2025-12-04T20:28:15.941012Z","shell.execute_reply.started":"2025-12-04T20:28:15.933931Z","shell.execute_reply":"2025-12-04T20:28:15.940384Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Top 20% Most Similar Users to 5557585400 (using DS):\n            similarity  common_products   DF   DS\nuser_id                                          \nB007J7MQ54        0.41                1 0.17 0.07\nB0080OK7YC        0.27                1 0.17 0.04\nB008ZVVHXU        0.22                1 0.17 0.04\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# ----------------------------\n# 1. Select target user's unrated products\n# ----------------------------\ntarget_ratings = user_item_matrix.loc[target_user]\nunrated_products = target_ratings[target_ratings == 0].index\n\n# ----------------------------\n# 2. Predict ratings using Top DS users\n# ----------------------------\npredictions = {}\nfor product in unrated_products:\n    # Get ratings of top DS users for this product\n    ratings_by_top_users = user_item_matrix.loc[top_users_DS.index, product]\n    weights = top_users_DS['DS']\n    \n    mask = ratings_by_top_users > 0  # only consider non-zero ratings\n    if mask.sum() > 0:\n        weighted_avg = np.dot(ratings_by_top_users[mask], weights[mask]) / weights[mask].sum()\n        predictions[product] = weighted_avg\n    else:\n        # fallback: use average rating of the product from all users\n        product_avg = user_item_matrix[product][user_item_matrix[product] > 0].mean()\n        if not np.isnan(product_avg):\n            predictions[product] = product_avg\n\n# ----------------------------\n# 3. Convert to DataFrame and sort\n# ----------------------------\npredicted_ratings_df = pd.DataFrame.from_dict(predictions, orient='index', columns=['predicted_rating'])\npredicted_ratings_df = predicted_ratings_df.sort_values(by='predicted_rating', ascending=False)\n\nprint(f\"\\nâœ… Predicted ratings for unrated products by {target_user}:\")\nprint(predicted_ratings_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:15.942532Z","iopub.execute_input":"2025-12-04T20:28:15.942781Z","iopub.status.idle":"2025-12-04T20:28:28.219637Z","shell.execute_reply.started":"2025-12-04T20:28:15.942765Z","shell.execute_reply":"2025-12-04T20:28:28.218988Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Predicted ratings for unrated products by 5557585400:\n                      predicted_rating\nAZZI8LIBZYYIK                     5.00\nAZYL2RTHUWR0P                     5.00\nAZY0M1ANDSEPL                     5.00\nA0108129TLIKAX34M8AA              5.00\nA01241534EPLP5O3KOP5              5.00\nA0234545X30ULJHGZUA3              5.00\nA0698182HBSA2D2MEX40              5.00\nA1006TXWG76H0N                    5.00\nA1008539FMIMQDV67RVY              5.00\nA100F1KITTQKKI                    5.00\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"**Case Study 2**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ----------------------------\n# 1. Load dataset & sample\n# ----------------------------\nDATASET_PATH = \"/kaggle/input/digital-music-csv/Digital_Music.csv\"\nratings = pd.read_csv(\n    DATASET_PATH,\n    header=None,\n    names=['user_id', 'product_id', 'rating', 'timestamp']\n)\n\n# Very small sample to allow variety\nratings_sample = ratings.sample(n=20000, random_state=42)\n\n# Filter users/products with few ratings\nmin_user_ratings = 1\nmin_item_ratings = 1\nuser_counts = ratings_sample['user_id'].value_counts()\nitem_counts = ratings_sample['product_id'].value_counts()\n\nratings_filtered = ratings_sample[\n    (ratings_sample['user_id'].isin(user_counts[user_counts >= min_user_ratings].index)) &\n    (ratings_sample['product_id'].isin(item_counts[item_counts >= min_item_ratings].index))\n]\n\n# ----------------------------\n# 2. Create User-Item Matrix\n# ----------------------------\nuser_item_matrix = ratings_filtered.pivot_table(\n    index='user_id',\n    columns='product_id',\n    values='rating',\n    aggfunc='mean'\n).fillna(0)\n\nuser_item_matrix = user_item_matrix.loc[(user_item_matrix.sum(axis=1) > 0),\n                                        (user_item_matrix.sum(axis=0) > 0)]\n\n# ----------------------------\n# 3. Select target user (1-2 ratings)\n# ----------------------------\ntarget_user_candidates = user_item_matrix[user_item_matrix.gt(0).sum(axis=1).between(1,2)].index\nif len(target_user_candidates) == 0:\n    raise ValueError(\"No user with 1-2 ratings. Ø­Ø§ÙˆÙ„ sample Ø£ÙƒØ¨Ø± Ø£Ùˆ Ø®ÙÙ min_user_ratings.\")\ntarget_user = target_user_candidates[0]\nprint(f\"âœ… Target user: {target_user} (rated {user_item_matrix.loc[target_user].gt(0).sum()} products)\")\n\n# ----------------------------\n# 4. Mean-center ratings\n# ----------------------------\nuser_means = user_item_matrix.replace(0, np.NaN).mean(axis=1)\nuser_item_centered = user_item_matrix.sub(user_means, axis=0).fillna(0)\n\n# ----------------------------\n# 5. Filter users with any overlap\n# ----------------------------\ntarget_ratings_centered = user_item_centered.loc[target_user]\nrated_products = target_ratings_centered[target_ratings_centered != 0].index\n\nsubset_users = user_item_centered[user_item_centered[rated_products].abs().sum(axis=1) > 0]\nsubset_users = subset_users.drop(index=target_user, errors='ignore')\nprint(f\"âœ… Users with overlap: {subset_users.shape[0]}\")\n\n# ----------------------------\n# 6. Compute Mean-Centered Cosine similarity\n# ----------------------------\nsimilarities = cosine_similarity(subset_users, target_ratings_centered.values.reshape(1, -1)).flatten()\nsimilarity_df = pd.DataFrame({\n    'similarity': similarities,\n    'common_products': (subset_users[rated_products] != 0).sum(axis=1)\n}, index=subset_users.index)\n\n# ----------------------------\n# 7. Discount Factor & DS\n# ----------------------------\nsimilarity_df['DF'] = similarity_df['common_products'] / len(rated_products)\nbeta_threshold = 0.05  # very low to include variety\nsimilarity_df = similarity_df[similarity_df['DF'] >= beta_threshold]\nsimilarity_df['DS'] = similarity_df['similarity'] * similarity_df['DF']\n\nsimilarity_df = similarity_df.sort_values(by='DS', ascending=False)\nprint(f\"\\nâœ… Top DS Users (Mean-Centered) for {target_user}:\")\nprint(similarity_df.head(20))\n\n# ----------------------------\n# 8. Users with DS < 1\n# ----------------------------\nvariety_users = similarity_df[similarity_df['DS'] < 1].sort_values(by='DS', ascending=False)\nprint(f\"\\nâœ… Users with DS < 1 for {target_user}:\")\nprint(variety_users.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:28.220502Z","iopub.execute_input":"2025-12-04T20:28:28.220816Z","iopub.status.idle":"2025-12-04T20:28:55.720409Z","shell.execute_reply.started":"2025-12-04T20:28:28.220780Z","shell.execute_reply":"2025-12-04T20:28:55.719511Z"}},"outputs":[{"name":"stdout","text":"âœ… Target user: 0001377647 (rated 1 products)\nâœ… Users with overlap: 15807\n\nâœ… Top DS Users (Mean-Centered) for 0001377647:\n            similarity  common_products   DF   DS\nuser_id                                          \nB001LM2ATA        1.00            19090 1.00 1.00\nB00DUZDMBG        1.00            19090 1.00 1.00\nB008BNARH4        1.00            19090 1.00 1.00\nB01731JB9O        1.00            19090 1.00 1.00\nB01BSA3V42        1.00            19090 1.00 1.00\nB0042U8T6G        1.00            19090 1.00 1.00\nB000Z7S5J6        1.00            19090 1.00 1.00\nB008B57V1M        1.00            19090 1.00 1.00\nB014M2BF2I        1.00            19090 1.00 1.00\nB00137QUTO        1.00            19090 1.00 1.00\nB000VZY9Z6        1.00            19090 1.00 1.00\nB004RSCT3W        1.00            19090 1.00 1.00\nB00JGEU1AU        1.00            19090 1.00 1.00\nB00AIDIQ3C        1.00            19090 1.00 1.00\nB000YNNTWY        1.00            19090 1.00 1.00\nB00433B2H0        1.00            19090 1.00 1.00\nB00137STFC        1.00            19090 1.00 1.00\nB00O4KOOL8        1.00            19090 1.00 1.00\nB01731IY0Q        1.00            19090 1.00 1.00\nB0037B8H4A        1.00            19090 1.00 1.00\n\nâœ… Users with DS < 1 for 0001377647:\n            similarity  common_products   DF   DS\nuser_id                                          \nB01GR6KH08        1.00            19090 1.00 1.00\nB000WCYFH0        1.00            19090 1.00 1.00\nB00P28XEEE        1.00            19090 1.00 1.00\nB00XTIFCXU        1.00            19090 1.00 1.00\nB0181O8VO8        1.00            19090 1.00 1.00\nB00000JFXN        1.00            19090 1.00 1.00\nB001BIJHIG        1.00            19090 1.00 1.00\nB00138KLGG        1.00            19090 1.00 1.00\nB01FRLCLAI        1.00            19090 1.00 1.00\nB000WKR696        1.00            19090 1.00 1.00\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ----------------------------\n# 1. Create synthetic dataset\n# ----------------------------\nnp.random.seed(42)\n\nusers = ['U1','U2','U3','U4','U5']\nproducts = ['P1','P2','P3','P4','P5','P6','P7','P8']\n\n# Random ratings 1-5\ndata = np.random.randint(1,6, size=(len(users), len(products)))\n\n# Force some zeros for variety\ndata[0, 3:] = 0   # target user has rated only first 3 products\ndata[1, 0:2] = 0  # user with partial overlap\ndata[2, 3:] = 0   # another partial overlap\ndata[3, :] = 5    # fully rated\ndata[4, 2:5] = 0  # partial overlap\n\nuser_item_matrix = pd.DataFrame(data, index=users, columns=products)\nprint(\"âœ… Synthetic User-Item Matrix:\")\nprint(user_item_matrix)\n\n# ----------------------------\n# 2. Mean-center\n# ----------------------------\nuser_means = user_item_matrix.replace(0, np.NaN).mean(axis=1)\nuser_item_centered = user_item_matrix.sub(user_means, axis=0).fillna(0)\n\n# ----------------------------\n# 3. Target user\n# ----------------------------\ntarget_user = 'U1'\ntarget_ratings = user_item_centered.loc[target_user]\nrated_products = target_ratings[target_ratings != 0].index\n\n# ----------------------------\n# 4. Users with overlap\n# ----------------------------\nsubset_users = user_item_centered[user_item_centered[rated_products].abs().sum(axis=1) > 0]\nsubset_users = subset_users.drop(index=target_user, errors='ignore')\n\n# ----------------------------\n# 5. Cosine similarity\n# ----------------------------\nsimilarities = cosine_similarity(subset_users, target_ratings.values.reshape(1,-1)).flatten()\nsimilarity_df = pd.DataFrame({\n    'similarity': similarities,\n    'common_products': (subset_users[rated_products] != 0).sum(axis=1)\n}, index=subset_users.index)\n\n# ----------------------------\n# 6. Discount Factor (DF) and DS\n# ----------------------------\nsimilarity_df['DF'] = similarity_df['common_products'] / len(rated_products)\nsimilarity_df['DS'] = similarity_df['similarity'] * similarity_df['DF']\n\n# Sort by DS\nsimilarity_df = similarity_df.sort_values(by='DS', ascending=False)\nprint(\"\\nâœ… DS values for target user:\")\nprint(similarity_df)\n\n# ----------------------------\n# 7. Show variety: DS=1 and DS<1\nvariety_users = similarity_df[similarity_df['DS'] < 1]\nprint(\"\\nâœ… Users with DS < 1:\")\nprint(variety_users)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:55.721321Z","iopub.execute_input":"2025-12-04T20:28:55.721604Z","iopub.status.idle":"2025-12-04T20:28:55.772714Z","shell.execute_reply.started":"2025-12-04T20:28:55.721580Z","shell.execute_reply":"2025-12-04T20:28:55.771980Z"}},"outputs":[{"name":"stdout","text":"âœ… Synthetic User-Item Matrix:\n    P1  P2  P3  P4  P5  P6  P7  P8\nU1   4   5   3   0   0   0   0   0\nU2   0   0   4   3   5   2   4   2\nU3   4   5   1   0   0   0   0   0\nU4   5   5   5   5   5   5   5   5\nU5   4   1   0   0   0   5   1   2\n\nâœ… DS values for target user:\n    similarity  common_products   DF    DS\nU3        0.97                7 1.00  0.97\nU5        0.40                7 1.00  0.40\nU2       -0.03                7 1.00 -0.03\n\nâœ… Users with DS < 1:\n    similarity  common_products   DF    DS\nU3        0.97                7 1.00  0.97\nU5        0.40                7 1.00  0.40\nU2       -0.03                7 1.00 -0.03\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ----------------------------\n# 1. Create larger synthetic dataset\n# ----------------------------\nnp.random.seed(42)\n\nusers = [f'U{i}' for i in range(1, 11)]       # 10 users\nproducts = [f'P{j}' for j in range(1, 13)]    # 12 products\n\n# Random ratings 1-5\ndata = np.random.randint(1, 6, size=(len(users), len(products)))\n\n# Force some zeros for variety\ndata[0, 6:] = 0   # target user rated only first 6 products\ndata[1, 0:3] = 0\ndata[2, 4:] = 0\ndata[3, :] = 5    # fully rated, high similarity\ndata[4, 2:6] = 0\ndata[5, 0:2] = 0\ndata[6, 5:] = 0\ndata[7, :] = 3    # fully rated but lower than 5\ndata[8, 1:4] = 0\ndata[9, 0:1] = 0\n\nuser_item_matrix = pd.DataFrame(data, index=users, columns=products)\nprint(\"âœ… Synthetic User-Item Matrix:\")\nprint(user_item_matrix)\n\n# ----------------------------\n# 2. Mean-center\n# ----------------------------\nuser_means = user_item_matrix.replace(0, np.NaN).mean(axis=1)\nuser_item_centered = user_item_matrix.sub(user_means, axis=0).fillna(0)\n\n# ----------------------------\n# 3. Target user\n# ----------------------------\ntarget_user = 'U1'\ntarget_ratings = user_item_centered.loc[target_user]\nrated_products = target_ratings[target_ratings != 0].index\n\n# ----------------------------\n# 4. Users with overlap\n# ----------------------------\nsubset_users = user_item_centered[user_item_centered[rated_products].abs().sum(axis=1) > 0]\nsubset_users = subset_users.drop(index=target_user, errors='ignore')\n\n# ----------------------------\n# 5. Compute Cosine similarity\n# ----------------------------\nsimilarities = cosine_similarity(subset_users, target_ratings.values.reshape(1,-1)).flatten()\nsimilarity_df = pd.DataFrame({\n    'similarity': similarities,\n    'common_products': (subset_users[rated_products] != 0).sum(axis=1)\n}, index=subset_users.index)\n\n# ----------------------------\n# 6. Discount Factor (DF) and DS\n# ----------------------------\nsimilarity_df['DF'] = similarity_df['common_products'] / len(rated_products)\nsimilarity_df['DS'] = similarity_df['similarity'] * similarity_df['DF']\n\n# Sort by DS\nsimilarity_df = similarity_df.sort_values(by='DS', ascending=False)\nprint(\"\\nâœ… DS values for target user:\")\nprint(similarity_df)\n\n# ----------------------------\n# 7. Top 20% DS Users\ntop_20_percent_count = max(1, int(0.2 * len(similarity_df)))  # at least 1 user\ntop_users_20pct = similarity_df.head(top_20_percent_count)\n\nprint(f\"\\nâœ… Top 20% DS Users for {target_user}:\")\nprint(top_users_20pct)\n\n# ----------------------------\n# 8. Predict ratings for unrated products\npredictions_top20 = {}\ntop_users = top_users_20pct.index\nweights = top_users_20pct['DS']\n\nfor product in user_item_matrix.columns:\n    if user_item_matrix.loc[target_user, product] == 0:  # unrated\n        ratings_by_top = user_item_matrix.loc[top_users, product]\n        mask = ratings_by_top > 0\n        if mask.sum() > 0:\n            weighted_avg = np.dot(ratings_by_top[mask], weights[mask]) / weights[mask].sum()\n            predictions_top20[product] = weighted_avg\n\npredicted_ratings_top20_df = pd.DataFrame.from_dict(predictions_top20, orient='index', columns=['predicted_rating'])\npredicted_ratings_top20_df = predicted_ratings_top20_df.sort_values(by='predicted_rating', ascending=False)\n\nprint(f\"\\nâœ… Predicted ratings for unrated products by {target_user} (Top 20% DS Users):\")\nprint(predicted_ratings_top20_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:55.773449Z","iopub.execute_input":"2025-12-04T20:28:55.773696Z","iopub.status.idle":"2025-12-04T20:28:55.808517Z","shell.execute_reply.started":"2025-12-04T20:28:55.773677Z","shell.execute_reply":"2025-12-04T20:28:55.807917Z"}},"outputs":[{"name":"stdout","text":"âœ… Synthetic User-Item Matrix:\n     P1  P2  P3  P4  P5  P6  P7  P8  P9  P10  P11  P12\nU1    4   5   3   5   5   2   0   0   0    0    0    0\nU2    0   0   0   2   4   5   1   4   2    5    4    1\nU3    1   3   3   2   0   0   0   0   0    0    0    0\nU4    5   5   5   5   5   5   5   5   5    5    5    5\nU5    2   4   0   0   0   0   3   1   4    2    4    2\nU6    0   0   5   2   2   4   2   2   4    4    1    5\nU7    5   2   5   2   1   0   0   0   0    0    0    0\nU8    3   3   3   3   3   3   3   3   3    3    3    3\nU9    5   0   0   0   4   1   4   2   1    5    3    4\nU10   0   3   1   3   5   3   1   5   2    3    1    2\n\nâœ… DS values for target user:\n     similarity  common_products   DF    DS\nU3         0.83               11 1.00  0.83\nU7         0.80               11 1.00  0.80\nU10        0.21               11 1.00  0.21\nU5         0.09               11 1.00  0.09\nU9         0.05               11 1.00  0.05\nU2         0.04               11 1.00  0.04\nU6        -0.10               11 1.00 -0.10\n\nâœ… Top 20% DS Users for U1:\n    similarity  common_products   DF   DS\nU3        0.83               11 1.00 0.83\n\nâœ… Predicted ratings for unrated products by U1 (Top 20% DS Users):\nEmpty DataFrame\nColumns: [predicted_rating]\nIndex: []\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"top_20_percent_count = max(1, int(0.2 * len(similarity_df)))  # at least 1 user\ntop_users_20pct = similarity_df.head(top_20_percent_count)\n\nprint(f\"\\nâœ… Top 20% DS Users for {target_user}:\")\nprint(top_users_20pct)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:55.809121Z","iopub.execute_input":"2025-12-04T20:28:55.809381Z","iopub.status.idle":"2025-12-04T20:28:55.814904Z","shell.execute_reply.started":"2025-12-04T20:28:55.809351Z","shell.execute_reply":"2025-12-04T20:28:55.814291Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Top 20% DS Users for U1:\n    similarity  common_products   DF   DS\nU3        0.83               11 1.00 0.83\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# ----------------------------\n# 8. Predict ratings for unrated products using top 20% DS users\n# ----------------------------\npredictions_top20 = {}\ntop_users = top_users_20pct.index\nweights = top_users_20pct['DS']\n\nfor product in user_item_matrix.columns:\n    if user_item_matrix.loc[target_user, product] == 0:  # unrated\n        ratings_by_top = user_item_matrix.loc[top_users, product]\n        mask = ratings_by_top > 0\n        if mask.sum() > 0:\n            weighted_avg = np.dot(ratings_by_top[mask], weights[mask]) / weights[mask].sum()\n            predictions_top20[product] = weighted_avg\n\npredicted_ratings_top20_df = pd.DataFrame.from_dict(predictions_top20, orient='index', columns=['predicted_rating'])\npredicted_ratings_top20_df = predicted_ratings_top20_df.sort_values(by='predicted_rating', ascending=False)\n\nprint(f\"\\nâœ… Predicted ratings for unrated products by {target_user} (Top 20% DS Users):\")\nprint(predicted_ratings_top20_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:55.815646Z","iopub.execute_input":"2025-12-04T20:28:55.816097Z","iopub.status.idle":"2025-12-04T20:28:55.830622Z","shell.execute_reply.started":"2025-12-04T20:28:55.816078Z","shell.execute_reply":"2025-12-04T20:28:55.830048Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Predicted ratings for unrated products by U1 (Top 20% DS Users):\nEmpty DataFrame\nColumns: [predicted_rating]\nIndex: []\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# ----------------------------\n# 1. Predict ratings for unrated products using top 20% DS users\n# ----------------------------\npredictions_top20 = {}\ntop_users = top_users_20pct.index\nweights = top_users_20pct['DS']\n\nfor product in user_item_matrix.columns:\n    if user_item_matrix.loc[target_user, product] == 0:\n        ratings_by_top = user_item_matrix.loc[top_users, product]\n        mask = ratings_by_top > 0\n        if mask.sum() > 0:\n            weighted_avg = np.dot(ratings_by_top[mask], weights[mask]) / weights[mask].sum()\n        else:\n            weighted_avg = user_item_matrix[product][user_item_matrix[product]>0].mean()  # fallback\n        predictions_top20[product] = weighted_avg\n\n\n# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame Ù…Ø±ØªØ¨Ø©\npredicted_ratings_top20_df = pd.DataFrame.from_dict(predictions_top20, orient='index', columns=['predicted_rating'])\npredicted_ratings_top20_df = predicted_ratings_top20_df.sort_values(by='predicted_rating', ascending=False)\n\nprint(f\"\\nâœ… Predicted ratings for unrated products by {target_user} (Top 20% DS Users):\")\nprint(predicted_ratings_top20_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:55.831321Z","iopub.execute_input":"2025-12-04T20:28:55.831492Z","iopub.status.idle":"2025-12-04T20:28:55.853799Z","shell.execute_reply.started":"2025-12-04T20:28:55.831479Z","shell.execute_reply":"2025-12-04T20:28:55.853062Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Predicted ratings for unrated products by U1 (Top 20% DS Users):\n     predicted_rating\nP10              3.86\nP8               3.14\nP12              3.14\nP9               3.00\nP11              3.00\nP7               2.71\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Î² threshold\nbeta_threshold = 0.3  # ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©\n\n# Ø­Ø³Ø§Ø¨ DF\nsimilarity_df['DF'] = similarity_df['common_products'] / len(rated_products)\n\n# Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ ÙÙ‚Ø· Ø¨Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† DF >= Î²\nsimilarity_df = similarity_df[similarity_df['DF'] >= beta_threshold]\n\n# Ø­Ø³Ø§Ø¨ DS\nsimilarity_df['DS'] = similarity_df['similarity'] * similarity_df['DF']\n\n# ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø­Ø³Ø¨ DS\nsimilarity_df = similarity_df.sort_values(by='DS', ascending=False)\n\nprint(\"\\nâœ… DF and DS values after applying Î² threshold:\")\nprint(similarity_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:55.854768Z","iopub.execute_input":"2025-12-04T20:28:55.855030Z","iopub.status.idle":"2025-12-04T20:28:55.874240Z","shell.execute_reply.started":"2025-12-04T20:28:55.855013Z","shell.execute_reply":"2025-12-04T20:28:55.873303Z"}},"outputs":[{"name":"stdout","text":"\nâœ… DF and DS values after applying Î² threshold:\n     similarity  common_products   DF    DS\nU3         0.83               11 1.00  0.83\nU7         0.80               11 1.00  0.80\nU10        0.21               11 1.00  0.21\nU5         0.09               11 1.00  0.09\nU9         0.05               11 1.00  0.05\nU2         0.04               11 1.00  0.04\nU6        -0.10               11 1.00 -0.10\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ Ø£Ø¹Ù„Ù‰ 20%\ntop_20_percent_count = max(1, int(0.2 * len(similarity_df)))  # Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 1 user\n\n# Ø§Ø®ØªÙŠØ§Ø± Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø­Ø³Ø¨ DS\ntop_users_20pct = similarity_df.head(top_20_percent_count)\n\nprint(f\"\\nâœ… Top 20% DS Users for target user:\")\nprint(top_users_20pct)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:55.875156Z","iopub.execute_input":"2025-12-04T20:28:55.875419Z","iopub.status.idle":"2025-12-04T20:28:55.889061Z","shell.execute_reply.started":"2025-12-04T20:28:55.875395Z","shell.execute_reply":"2025-12-04T20:28:55.888269Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Top 20% DS Users for target user:\n    similarity  common_products   DF   DS\nU3        0.83               11 1.00 0.83\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# ----------------------------\n# Predict ratings for unrated products using top DS users with fallback\n# ----------------------------\npredictions_top20 = {}\ntop_users = top_users_20pct.index\nweights = top_users_20pct['DS']\n\nfor product in user_item_matrix.columns:\n    if user_item_matrix.loc[target_user, product] == 0:  # unrated\n        ratings_by_top = user_item_matrix.loc[top_users, product]\n        mask = ratings_by_top > 0\n        if mask.sum() > 0:\n            # Weighted average Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DS\n            weighted_avg = np.dot(ratings_by_top[mask], weights[mask]) / weights[mask].sum()\n        else:\n            # Fallback: Ù…ØªÙˆØ³Ø· ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ù†ØªØ¬ Ø¹Ø§Ù„Ù…ÙŠÙ‹Ø§\n            weighted_avg = user_item_matrix[product][user_item_matrix[product] > 0].mean()\n        predictions_top20[product] = weighted_avg\n\n# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame Ù…Ø±ØªØ¨Ø©\npredicted_ratings_top20_df = pd.DataFrame.from_dict(\n    predictions_top20, orient='index', columns=['predicted_rating']\n)\npredicted_ratings_top20_df = predicted_ratings_top20_df.sort_values(\n    by='predicted_rating', ascending=False\n)\n\nprint(f\"\\nâœ… Predicted ratings for unrated products by {target_user} (Top DS Users with fallback):\")\nprint(predicted_ratings_top20_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:28:55.889927Z","iopub.execute_input":"2025-12-04T20:28:55.890370Z","iopub.status.idle":"2025-12-04T20:28:55.909863Z","shell.execute_reply.started":"2025-12-04T20:28:55.890351Z","shell.execute_reply":"2025-12-04T20:28:55.909296Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Predicted ratings for unrated products by U1 (Top DS Users with fallback):\n     predicted_rating\nP10              3.86\nP8               3.14\nP12              3.14\nP9               3.00\nP11              3.00\nP7               2.71\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"**Case Study 3**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# 1. Load Data\nDATASET_PATH = \"/kaggle/input/digital-music-csv/Digital_Music.csv\"\nratings = pd.read_csv(DATASET_PATH, header=None, names=['user_id', 'product_id', 'rating', 'timestamp'])\n\n# ---------------------------------------------------------\n# Ø®Ø·ÙˆØ© 1: ØªÙ†Ø¸ÙŠÙ Ø°ÙƒÙŠ (Ù†Ø£Ø®Ø° ÙÙ‚Ø· Ù…Ù† Ù‚ÙŠÙ…ÙˆØ§ Ø£ÙƒØ«Ø± Ù…Ù† 10 Ù…Ù†ØªØ¬Ø§Øª)\n# Ù„Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† \"Ø­Ù‚ÙŠÙ‚ÙŠÙŠÙ†\" ÙˆÙ„ÙŠØ³ Ø¹Ø´ÙˆØ§Ø¦ÙŠÙŠÙ†\n# ---------------------------------------------------------\nuser_counts = ratings['user_id'].value_counts()\nactive_users = user_counts[user_counts >= 10].index\nratings_filtered = ratings[ratings['user_id'].isin(active_users)]\n\n# Ù†Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ù†Ø´Ø·ÙŠÙ† ÙÙ‚Ø·\nratings_sample = ratings_filtered.head(100000) \n\n# ---------------------------------------------------------\n# Ø®Ø·ÙˆØ© 2: Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ØµÙÙˆÙØ©\n# ---------------------------------------------------------\nuser_item_matrix = ratings_sample.pivot_table(\n    index='user_id', \n    columns='product_id', \n    values='rating'\n) # Ù„Ø§ ØªØ¶Ø¹ fillna(0) Ù‡Ù†Ø§ Ù„Ø£Ù†Ù†Ø§ Ù†Ø­ØªØ§Ø¬ NaN Ù„Ø­Ø³Ø§Ø¨ Ø¨ÙŠØ±Ø³ÙˆÙ† Ø§Ù„ØµØ­ÙŠØ­\n\n# Ø§Ø®ØªÙŠØ§Ø± Ù‡Ø¯Ù Ù„Ø¯ÙŠÙ‡ ØªÙ‚ÙŠÙŠÙ…Ø§Øª ÙƒØ«ÙŠØ±Ø©\ntarget_user = user_item_matrix.count(axis=1).idxmax()\nprint(f\"Target User: {target_user} (Rated {user_item_matrix.loc[target_user].count()} items)\")\n\n# ---------------------------------------------------------\n# Ø®Ø·ÙˆØ© 3: Ø­Ø³Ø§Ø¨ PCC Ù…Ø¹ \"Ø¹Ù‚Ø§Ø¨\" Ù„Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ù„ÙŠÙ„ (Shrinkage)\n# ---------------------------------------------------------\ntarget_ratings = user_item_matrix.loc[target_user]\npcc_dict = {}\n\n# Ø¹Ø§Ù…Ù„ Ø§Ù„Ø¹Ù‚Ø§Ø¨: ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ Ø§Ù„Ø±Ù‚Ù…ØŒ Ø¹Ø§Ù‚Ø¨Ù†Ø§ Ø§Ù„ØªØ¯Ø§Ø®Ù„ Ø§Ù„Ù‚Ù„ÙŠÙ„ Ø£ÙƒØ«Ø±\nSHRINKAGE_PARAMETER = 5  \n\nfor user in user_item_matrix.index:\n    if user == target_user:\n        continue\n    \n    other_ratings = user_item_matrix.loc[user]\n    \n    # Ù†Ø£Ø®Ø° ÙÙ‚Ø· Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ Ù‚ÙŠÙ…Ù‡Ø§ Ø§Ù„Ø§Ø«Ù†Ø§Ù† (Ù„ÙŠØ³Øª NaN Ø¹Ù†Ø¯ Ø§Ù„Ø·Ø±ÙÙŠÙ†)\n    common = pd.concat([target_ratings, other_ratings], axis=1, join='inner').dropna()\n    n_common = len(common)\n    \n    # Ø§Ù„Ø´Ø±Ø·: ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ´ØªØ±ÙƒÙˆØ§ ÙÙŠ 3 Ù…Ù†ØªØ¬Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„\n    if n_common < 3:\n        continue\n\n    x = common.iloc[:, 0].values\n    y = common.iloc[:, 1].values\n\n    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø«Ø§Ø¨ØªØ© (Ù…Ø«Ù„Ø§Ù‹ ÙƒÙ„Ù‡Ø§ 5)ØŒ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ 0 ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·\n    if np.std(x) == 0 or np.std(y) == 0:\n        continue\n\n    # Ø­Ø³Ø§Ø¨ Ø¨ÙŠØ±Ø³ÙˆÙ† Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ\n    corr = np.corrcoef(x, y)[0, 1]\n    \n    # ğŸ”¥ Ù‡Ù†Ø§ Ø§Ù„Ø­Ù„ Ø§Ù„Ø³Ø­Ø±ÙŠ: ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ§Øª\n    # Ù„Ùˆ Ø§Ø´ØªØ±ÙƒÙˆØ§ ÙÙŠ 3 Ù…Ù†ØªØ¬Ø§Øª ÙÙ‚Ø·: Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø³ÙŠØ¶Ø±Ø¨ ÙÙŠ (3 / 8) = 0.37 (ÙŠÙ‚Ù„Ù„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¬Ø¯Ø§Ù‹)\n    # Ù„Ùˆ Ø§Ø´ØªØ±ÙƒÙˆØ§ ÙÙŠ 50 Ù…Ù†ØªØ¬: Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø³ÙŠØ¶Ø±Ø¨ ÙÙŠ (50 / 55) = 0.90 (ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡)\n    weighted_corr = corr * (n_common / (n_common + SHRINKAGE_PARAMETER))\n    \n    pcc_dict[user] = {\n        'Sim': weighted_corr, \n        'Raw_Corr': corr, \n        'Common_Items': n_common\n    }\n\n# ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ù†ØªØ§Ø¦Ø¬\nresults = pd.DataFrame.from_dict(pcc_dict, orient='index')\n\nif not results.empty:\n    results = results.sort_values(by='Sim', ascending=False)\n    print(\"\\nâœ… Top Similar Users (Weighted PCC):\")\n    print(results.head(10))\nelse:\n    print(\"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø­Ù‚Ù‚ÙˆØ§ Ø§Ù„Ø´Ø±ÙˆØ· (Ø¬Ø±Ø¨ ØªÙ‚Ù„ÙŠÙ„ Ø´Ø±Ø· n_common Ø£Ùˆ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹ÙŠÙ†Ø©)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:51:25.396425Z","iopub.execute_input":"2025-12-04T20:51:25.397393Z","iopub.status.idle":"2025-12-04T20:52:13.510055Z","shell.execute_reply.started":"2025-12-04T20:51:25.397359Z","shell.execute_reply":"2025-12-04T20:52:13.509415Z"}},"outputs":[{"name":"stdout","text":"Target User: B000V6ADNW (Rated 1058 items)\n\nâœ… Top Similar Users (Weighted PCC):\n             Sim  Raw_Corr  Common_Items\nB000VRTJI6  0.56      0.95             7\nB0014JIVWA  0.22      0.49             4\nB000W11N6W  0.11      0.21             5\nB00122909W  0.09      0.15             8\nB001237HCI -0.12     -0.25             5\nB0011YBRFQ -0.19     -0.50             3\nB00122KC4E -0.26     -0.58             4\nB000W0TIRE -0.29     -0.66             4\n9714721180 -0.37     -0.46            21\nB000WKT6B2 -0.37     -1.00             3\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ----------------------------\n# 1. Load & Filter Data (Smart Sampling)\n# ----------------------------\nDATASET_PATH = \"/kaggle/input/digital-music-csv/Digital_Music.csv\"\nratings = pd.read_csv(DATASET_PATH, header=None, names=['user_id', 'product_id', 'rating', 'timestamp'])\n\n# ØªØµÙÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ«ÙŠÙØ© (Dense) Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£ØµÙØ§Ø±\n# Ù†Ø£Ø®Ø° Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† Ù‚ÙŠÙ…ÙˆØ§ 10 Ù…Ù†ØªØ¬Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„\nuser_counts = ratings['user_id'].value_counts()\nactive_users = user_counts[user_counts >= 10].index\nratings_filtered = ratings[ratings['user_id'].isin(active_users)]\n\n# Ù†Ø£Ø®Ø° Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªÙ‚ÙŠÙŠÙ…Ù‡Ø§ 10 Ù…Ø±Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„\nitem_counts = ratings_filtered['product_id'].value_counts()\npopular_items = item_counts[item_counts >= 10].index\nratings_final = ratings_filtered[ratings_filtered['product_id'].isin(popular_items)]\n\n# Ù†Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© (Ù…Ø«Ù„Ø§Ù‹ Ø£ÙˆÙ„ 1000 Ù…Ø³ØªØ®Ø¯Ù… Ù†Ø´Ø· Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©)\n# ÙŠÙ…ÙƒÙ†Ùƒ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø±Ù‚Ù… Ø­Ø³Ø¨ Ù‚ÙˆØ© Ø¬Ù‡Ø§Ø²Ùƒ\nunique_users = ratings_final['user_id'].unique()[:1000] \nratings_final = ratings_final[ratings_final['user_id'].isin(unique_users)]\n\nprint(f\"âœ… Data Shape: {ratings_final.shape} | Users: {len(unique_users)}\")\n\n# ----------------------------\n# 2. Create User-Item Matrix\n# ----------------------------\nuser_item_matrix = ratings_final.pivot_table(\n    index='user_id', \n    columns='product_id', \n    values='rating'\n).fillna(0)\n\n# ----------------------------\n# 3. Mean-Centered Cosine Similarity (Vectorized Pearson)\n# ----------------------------\n# Ø·Ø±Ø­ Ø§Ù„Ù…ØªÙˆØ³Ø· Ù…Ù† ÙƒÙ„ ØµÙ (Ù…Ø¹ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£ØµÙØ§Ø± ÙÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£ØµÙ„ÙŠ)\n# Ù‡Ù†Ø§ Ø³Ù†Ø·Ø±Ø­ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø¨Ø¨Ø³Ø§Ø·Ø© (ØªÙ‚Ø±ÙŠØ¨ Ù…Ù‚Ø¨ÙˆÙ„ Ù„Ù„Ø³Ø±Ø¹Ø©)\nuser_means = user_item_matrix.mean(axis=1)\nmatrix_centered = user_item_matrix.sub(user_means, axis=0)\n\n# Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ (N x N Matrix)\nsim_matrix = cosine_similarity(matrix_centered)\n\n# ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ù€ DataFrame Ù„ÙŠØ³Ù‡Ù„ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡\nsim_df = pd.DataFrame(sim_matrix, index=user_item_matrix.index, columns=user_item_matrix.index)\n\n# ----------------------------\n# 4. Get Top 20% Neighbors for EACH User\n# ----------------------------\ntop_20_percent_dict = {}\n\n# Ø¹Ø¯Ø¯ Ø§Ù„Ø¬ÙŠØ±Ø§Ù† Ø§Ù„ÙƒÙ„ÙŠ (Ù†Ø·Ø±Ø­ 1 Ù„Ø£Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø§ ÙŠØ­Ø³Ø¨ Ù…Ø¹ Ù†ÙØ³Ù‡)\ntotal_neighbors = len(sim_df) - 1\nk_top = int(total_neighbors * 0.20)  # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù€ 20%\n\nprint(f\"ğŸ” Finding top {k_top} neighbors (20%) for each user...\\n\")\n\nfor user in sim_df.index:\n    # 1. Ø¬Ù„Ø¨ ØµÙ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…\n    user_similarities = sim_df.loc[user]\n    \n    # 2. Ø­Ø°Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù†ÙØ³Ù‡ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© (Ù„Ø£Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ 1.0)\n    user_similarities = user_similarities.drop(user)\n    \n    # 3. Ø§Ù„ØªØ±ØªÙŠØ¨ ØªÙ†Ø§Ø²Ù„ÙŠØ§Ù‹ (Ø§Ù„Ø£Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹)\n    sorted_sim = user_similarities.sort_values(ascending=False)\n    \n    # 4. Ø£Ø®Ø° Ø£ÙˆÙ„ 20%\n    top_neighbors = sorted_sim.head(k_top)\n    \n    # Ø§Ù„ØªØ®Ø²ÙŠÙ†\n    top_20_percent_dict[user] = top_neighbors.index.tolist()\n\n# ----------------------------\n# 5. Show Results Example\n# ----------------------------\n# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ø£ÙˆÙ„ 3 Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙ‚Ø· ÙƒØ¹ÙŠÙ†Ø©\nfor i, (user, neighbors) in enumerate(top_20_percent_dict.items()):\n    if i >= 3: break\n    print(f\"ğŸ‘¤ User: {user}\")\n    print(f\"   -> Top 20% Neighbors ({len(neighbors)} users): {neighbors[:5]} ...\")\n    print(\"-\" * 50)\n\n# Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù\n# output_df = pd.DataFrame.from_dict(top_20_percent_dict, orient='index')\n# output_df.to_csv(\"top_20_percent_neighbors.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:54:01.632200Z","iopub.execute_input":"2025-12-04T20:54:01.633141Z","iopub.status.idle":"2025-12-04T20:54:04.876280Z","shell.execute_reply.started":"2025-12-04T20:54:01.633109Z","shell.execute_reply":"2025-12-04T20:54:04.875403Z"}},"outputs":[{"name":"stdout","text":"âœ… Data Shape: (8412, 4) | Users: 1000\nğŸ” Finding top 199 neighbors (20%) for each user...\n\nğŸ‘¤ User: 0001377647\n   -> Top 20% Neighbors (199 users): ['B000TDWSYO', 'B000QVXDR0', 'B000VZO4TW', 'B000S56380', 'B000T1EJ0W'] ...\n--------------------------------------------------\nğŸ‘¤ User: 0001388703\n   -> Top 20% Neighbors (199 users): ['B000VZJS84', '5557386823', 'B000V697Z2', 'B000GSACA4', 'B000TE3JTQ'] ...\n--------------------------------------------------\nğŸ‘¤ User: 0006920055\n   -> Top 20% Neighbors (199 users): ['5557386823', 'B000TE3JTQ', '0001388703', 'B000GSACA4', 'B0009Z0SRG'] ...\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ----------------------------\n# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØªØµÙÙŠØªÙ‡ (Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©)\n# ----------------------------\nprint(\"â³ Loading Original Dataset...\")\nDATASET_PATH = \"/kaggle/input/digital-music-csv/Digital_Music.csv\"\nratings = pd.read_csv(\n    DATASET_PATH, \n    header=None, \n    names=['user_id', 'product_id', 'rating', 'timestamp']\n)\n\nprint(f\"Original Data Size: {ratings.shape}\")\n\n# --- Ø®Ø·ÙˆØ© 1: ØªØµÙÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù… ---\n# Ø³Ù†Ø­ØªÙØ¸ ÙÙ‚Ø· Ø¨Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† Ù‚ÙŠÙ…ÙˆØ§ 20 Ù…Ù†ØªØ¬Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„\n# ÙˆØ§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ Ø­ØµÙ„Øª Ø¹Ù„Ù‰ 20 ØªÙ‚ÙŠÙŠÙ…Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ (Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙ‚Ø§Ø·Ø¹)\nmin_user_ratings = 20\nmin_item_ratings = 20\n\n# ØªØµÙÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†\nuser_counts = ratings['user_id'].value_counts()\nactive_users = user_counts[user_counts >= min_user_ratings].index\nratings_filtered = ratings[ratings['user_id'].isin(active_users)]\n\n# ØªØµÙÙŠØ© Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª (Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©)\nitem_counts = ratings_filtered['product_id'].value_counts()\npopular_items = item_counts[item_counts >= min_item_ratings].index\nratings_final = ratings_filtered[ratings_filtered['product_id'].isin(popular_items)]\n\n# ØªØ­Ø°ÙŠØ±: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø§ ØªØ²Ø§Ù„ Ø¶Ø®Ù…Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ù†Ø£Ø®Ø° Ø£Ø¹Ù„Ù‰ 1000 Ù…Ø³ØªØ®Ø¯Ù… Ù†Ø´Ø§Ø·Ø§Ù‹ ÙÙ‚Ø·\nif len(ratings_final['user_id'].unique()) > 2000:\n    top_2000_users = ratings_final['user_id'].value_counts().head(2000).index\n    ratings_final = ratings_final[ratings_final['user_id'].isin(top_2000_users)]\n\nprint(f\"âœ… Filtered Data Size: {ratings_final.shape}\")\nprint(f\"   Users: {ratings_final['user_id'].nunique()} | Items: {ratings_final['product_id'].nunique()}\")\n\n# ----------------------------\n# 2. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ØµÙÙˆÙØ© (User-Item Matrix)\n# ----------------------------\nuser_item_matrix = ratings_final.pivot_table(\n    index='user_id', \n    columns='product_id', \n    values='rating'\n).fillna(0)\n\n# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù„ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù… (Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø£ØµÙØ§Ø±)\ntrue_user_means = user_item_matrix.replace(0, np.nan).mean(axis=1)\n\n# ----------------------------\n# 3. Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙŠØ±Ø§Ù† (Top 20%)\n# ----------------------------\n# Mean-Centered Matrix (User - Mean)\nmatrix_centered = user_item_matrix.sub(true_user_means, axis=0).fillna(0)\n\n# Cosine Similarity (ÙŠÙƒØ§ÙØ¦ Pearson Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ø±Ø­)\nsim_matrix = cosine_similarity(matrix_centered)\nsim_df = pd.DataFrame(sim_matrix, index=user_item_matrix.index, columns=user_item_matrix.index)\n\n# ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ 20% Ø¬ÙŠØ±Ø§Ù† Ù„ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù…\nneighbors_dict = {}\nk_percentage = 0.20 # Ù†Ø³Ø¨Ø© 20%\ntotal_candidates = len(sim_df) - 1 # Ø§Ù„ÙƒÙ„ Ù…Ø§ Ø¹Ø¯Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù†ÙØ³Ù‡\nk_neighbors = max(1, int(total_candidates * k_percentage))\n\nprint(f\"ğŸ” Finding top {k_neighbors} neighbors (20%) per user...\")\n\nfor user in sim_df.index:\n    # ØªØ±ØªÙŠØ¨ Ø§Ù„ØªÙ†Ø§Ø²Ù„ÙŠ ÙˆØ§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù†ÙØ³Ù‡\n    user_sims = sim_df.loc[user].drop(user).sort_values(ascending=False)\n    # Ø£Ø®Ø° Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©\n    top_neighbors = user_sims.head(k_neighbors).index.tolist()\n    neighbors_dict[user] = top_neighbors\n\n# ----------------------------\n# 4. Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© (Rating Prediction)\n# ----------------------------\npredictions = []\n\nprint(\"ğŸš€ Starting Prediction Loop (Active Users Only)...\")\n\n# Ù…ØµÙÙˆÙØ© Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª (Rating - Mean) Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø¨Ø³Ø±Ø¹Ø©\nratings_diff_matrix = user_item_matrix.replace(0, np.nan).sub(true_user_means, axis=0)\n\n# Ù„Ù„ØªØ³Ø±ÙŠØ¹: Ø³Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ø£ÙˆÙ„ 10 Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙ‚Ø· ÙƒØ¹ÙŠÙ†Ø© (Ù„Ø£Ù† Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù„ÙƒÙ„ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹)\n# ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø²Ø§Ù„Ø© [:10] Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ÙƒÙ„\ntarget_users_sample = user_item_matrix.index[:10] \n\nfor target_user in target_users_sample:\n    \n    # 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙŠØ±Ø§Ù†\n    my_neighbors = neighbors_dict[target_user]\n    \n    # 2. Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ Ù„Ù… ÙŠÙ‚ÙŠÙ…Ù‡Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŸ (Ù‚ÙŠÙ…ØªÙ‡Ø§ 0)\n    user_row = user_item_matrix.loc[target_user]\n    unrated_items = user_row[user_row == 0].index\n    \n    # Ù„Ø§ Ù†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ù„ÙŠÙˆÙ† Ù…Ù†ØªØ¬ØŒ Ù†Ø®ØªØ§Ø± ÙÙ‚Ø· Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ Ù‚ÙŠÙ…Ù‡Ø§ Ø§Ù„Ø¬ÙŠØ±Ø§Ù†\n    # Ù†Ø£Ø®Ø° ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„Ø¬ÙŠØ±Ø§Ù† Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‚ÙŠÙ…Ø©\n    neighbors_ratings = user_item_matrix.loc[my_neighbors, unrated_items]\n    \n    # Ù†Ø£Ø®Ø° ÙÙ‚Ø· Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙˆØ¬Ø¯ Ù„Ù‡Ø§ ØªÙ‚ÙŠÙŠÙ… ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø¬ÙŠØ±Ø§Ù†\n    items_to_predict = neighbors_ratings.columns[neighbors_ratings.sum() > 0]\n    \n    for item in items_to_predict:\n        # Ù†Ø­ØªØ§Ø¬ Ø§Ù„Ø¬ÙŠØ±Ø§Ù† Ø§Ù„Ø°ÙŠÙ† Ù‚ÙŠÙ…ÙˆØ§ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†ØªØ¬ ÙØ¹Ù„ÙŠØ§Ù‹\n        # (Ratings > 0)\n        raters = neighbors_ratings.index[neighbors_ratings[item] > 0]\n        \n        if len(raters) == 0:\n            continue\n            \n        # Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©:\n        # Prediction = Mean_u + (Sum(Sim * (R_v - Mean_v)) / Sum(|Sim|))\n        \n        # Ø§Ù„Ø¨Ø³Ø·: Ø§Ù„ØªØ´Ø§Ø¨Ù‡ * (ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬Ø§Ø± - Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¬Ø§Ø±)\n        sim_scores = sim_df.loc[target_user, raters]\n        neighbor_diffs = ratings_diff_matrix.loc[raters, item] # Ø§Ù„Ù‚ÙŠÙ…Ø© Ø¬Ø§Ù‡Ø²Ø©\n        \n        numerator = np.dot(sim_scores, neighbor_diffs)\n        denominator = sim_scores.abs().sum()\n        \n        if denominator == 0:\n            continue\n            \n        predicted_rating = true_user_means[target_user] + (numerator / denominator)\n        \n        # Ø¶Ø¨Ø· Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ÙŠÙ† 1 Ùˆ 5\n        predicted_rating = np.clip(predicted_rating, 1, 5)\n        \n        predictions.append({\n            'User': target_user,\n            'Product': item,\n            'Predicted_Rating': round(predicted_rating, 2)\n        })\n\n# ----------------------------\n# 5. Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n# ----------------------------\npred_df = pd.DataFrame(predictions)\n\nif not pred_df.empty:\n    print(\"\\nâœ… Top 10 Predicted Ratings (User-Based CF):\")\n    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø£Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ… Ù…ØªÙˆÙ‚Ø¹\n    print(pred_df.sort_values(by='Predicted_Rating', ascending=False).head(10))\n    \n    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª\n    print(f\"\\nTotal Predictions Made: {len(pred_df)}\")\nelse:\n    print(\"No predictions made (Not enough overlap in this sample).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:59:16.407382Z","iopub.execute_input":"2025-12-04T20:59:16.408011Z","iopub.status.idle":"2025-12-04T20:59:25.931778Z","shell.execute_reply.started":"2025-12-04T20:59:16.407983Z","shell.execute_reply":"2025-12-04T20:59:25.931121Z"}},"outputs":[{"name":"stdout","text":"â³ Loading Original Dataset...\nOriginal Data Size: (1584082, 4)\nâœ… Filtered Data Size: (23565, 4)\n   Users: 2000 | Items: 1050\nğŸ” Finding top 399 neighbors (20%) per user...\nğŸš€ Starting Prediction Loop (Active Users Only)...\n\nâœ… Top 10 Predicted Ratings (User-Based CF):\n            User        Product  Predicted_Rating\n7032  B000S43HH6  AS0CCKP5ZB0M6              5.00\n7033  B000S43HH6  ASDPTCEGEINUZ              5.00\n7035  B000S43HH6  ASU3NPI7FLQBA              5.00\n7036  B000S43HH6  ASVNSWIXBV72Q              5.00\n7024  B000S43HH6  AQFW7GDBQ0DPE              5.00\n7025  B000S43HH6  AQHVR2V3YA1MM              5.00\n7026  B000S43HH6  AR3N1VLW722V9              5.00\n7027  B000S43HH6  AR8O7SVSEUI9Z              5.00\n7028  B000S43HH6  ARIXGO75TOE4K              5.00\n7029  B000S43HH6  ARQ2J7D4DZTRL              5.00\n\nTotal Predictions Made: 7074\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# ---------------------------------------------------------\n# 1. Load & Filter Original Data\n# ---------------------------------------------------------\n# We use smart filtering to keep the code fast and memory-safe\nprint(\"â³ Loading and Preprocessing...\")\n\nDATASET_PATH = \"/kaggle/input/digital-music-csv/Digital_Music.csv\"\nratings = pd.read_csv(DATASET_PATH, header=None, names=['user_id', 'product_id', 'rating', 'timestamp'])\n\n# Filter: Keep users with > 10 ratings and items with > 10 ratings\n# This ensures we actually have overlaps to calculate similarity\nmin_activity = 10\nuser_counts = ratings['user_id'].value_counts()\nactive_users = user_counts[user_counts >= min_activity].index\nratings = ratings[ratings['user_id'].isin(active_users)]\n\nitem_counts = ratings['product_id'].value_counts()\npopular_items = item_counts[item_counts >= min_activity].index\nratings = ratings[ratings['product_id'].isin(popular_items)]\n\n# For demonstration speed, let's take the top 1,000 active users\n# (You can increase this if you have more RAM)\ntop_users = ratings['user_id'].value_counts().head(1000).index\nratings_final = ratings[ratings['user_id'].isin(top_users)]\n\nprint(f\"âœ… Data Ready: {ratings_final.shape} | Users: {ratings_final['user_id'].nunique()}\")\n\n# ---------------------------------------------------------\n# 2. Create Matrix\n# ---------------------------------------------------------\n# Use float for NaN support\nuser_item_matrix = ratings_final.pivot_table(\n    index='user_id', \n    columns='product_id', \n    values='rating'\n)\n\n# ---------------------------------------------------------\n# 3. Define Discounted Similarity Function\n# ---------------------------------------------------------\ndef calculate_discounted_similarity(matrix_df, beta=1.0, shrinkage=5):\n    \"\"\"\n    Computes similarity based on:\n    1. Threshold Beta: Ignore items where |diff| > beta\n    2. Distance Penalty: Closer ratings = Higher score\n    3. Discounting (Shrinkage): Penalize low common item counts\n    \"\"\"\n    users = matrix_df.index\n    n_users = len(users)\n    sim_matrix = np.zeros((n_users, n_users))\n    \n    # Convert to numpy array for speed\n    # Shape: (Users, Items)\n    R = matrix_df.values\n    \n    print(f\"ğŸš€ Calculating Discounted Similarities (Beta={beta})...\")\n    \n    # Loop over users (optimized inner operations)\n    for i in range(n_users):\n        # User u vector\n        u_vec = R[i]\n        \n        # We compare User u against ALL other users at once (Vectorization)\n        # diffs shape: (n_users, n_items)\n        # abs(R - u_vec) calculates difference between User i and everyone else\n        diffs = np.abs(R - u_vec)\n        \n        # Masks\n        # 1. Valid ratings: Both u and v must have rated the item (not NaN)\n        # (np.isnan check is needed because we didn't fillna(0))\n        isnan_mask = np.isnan(diffs) \n        \n        # 2. Threshold check: Difference must be <= Beta\n        threshold_mask = diffs <= beta\n        \n        # Final mask: Valid ratings AND within threshold\n        # items that are NaN will be False in threshold_mask anyway if strictly handled, \n        # but explicit check is safer.\n        valid_mask = (~np.isnan(R)) & (~np.isnan(u_vec)) & threshold_mask\n        \n        # Count common items passing the threshold\n        common_counts = np.sum(valid_mask, axis=1)\n        \n        # Calculate raw similarity sum for valid items\n        # Score per item = 1 - (diff / (beta + 1))\n        # If diff=0 -> score=1. If diff=1 (beta) -> score=0.5\n        item_scores = 1 - (diffs / (beta + 1))\n        \n        # Zero out invalid items\n        item_scores[~valid_mask] = 0\n        \n        # Sum of scores per user\n        sum_scores = np.sum(item_scores, axis=1)\n        \n        # Average Similarity (Sum / Count)\n        # Avoid division by zero\n        avg_sim = np.zeros_like(sum_scores)\n        nonzero_mask = common_counts > 0\n        avg_sim[nonzero_mask] = sum_scores[nonzero_mask] / common_counts[nonzero_mask]\n        \n        # --- DISCOUNTING STEP ---\n        # Apply Shrinkage: Sim * (n / (n + shrinkage))\n        discount_factor = common_counts / (common_counts + shrinkage)\n        final_sim = avg_sim * discount_factor\n        \n        sim_matrix[i] = final_sim\n        \n    # Set diagonal to 1 (or 0 if we want to ignore self)\n    np.fill_diagonal(sim_matrix, 1.0)\n    \n    return pd.DataFrame(sim_matrix, index=users, columns=users)\n\n# Run Calculation\n# Beta=1: Tolerates a difference of 1 star.\nsim_df = calculate_discounted_similarity(user_item_matrix, beta=1.0, shrinkage=3)\n\n# ---------------------------------------------------------\n# 4. Identify Top 20% Neighbors\n# ---------------------------------------------------------\ntop_20_neighbors = {}\ntotal_candidates = len(sim_df) - 1\nk_top = max(1, int(total_candidates * 0.20)) # 20% count\n\nprint(f\"\\nğŸ” Extracting top {k_top} neighbors (20%) per user...\")\n\nfor user in sim_df.index:\n    # 1. Get row, drop self\n    user_sims = sim_df.loc[user].drop(user)\n    \n    # 2. Filter out users with 0 similarity (no valid overlap)\n    user_sims = user_sims[user_sims > 0]\n    \n    # 3. Sort Descending\n    user_sims = user_sims.sort_values(ascending=False)\n    \n    # 4. Slice top 20%\n    # If valid neighbors are fewer than k_top, we take what we have\n    current_k = min(len(user_sims), k_top)\n    best_neighbors = user_sims.head(current_k).index.tolist()\n    \n    top_20_neighbors[user] = best_neighbors\n\n# ---------------------------------------------------------\n# 5. Display Results\n# ---------------------------------------------------------\nprint(\"\\nâœ… Results (Discounted Distance-Based Similarity):\")\ncount = 0\nfor user, neighbors in top_20_neighbors.items():\n    if len(neighbors) > 0:\n        print(f\"User {user}:\")\n        # Print top 5 just to see scores\n        top_5_scores = sim_df.loc[user, neighbors[:5]].values\n        print(f\"   -> Top Neighbors: {neighbors[:5]}...\")\n        print(f\"   -> Scores: {np.round(top_5_scores, 3)}\")\n        count += 1\n    if count >= 5: break\n\n# Stats\nneighbor_counts = [len(n) for n in top_20_neighbors.values()]\nprint(f\"\\nAverage neighbors found per user: {np.mean(neighbor_counts):.1f}\")\nprint(f\"Total Users processed: {len(top_20_neighbors)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T21:32:28.550303Z","iopub.execute_input":"2025-12-04T21:32:28.550634Z","iopub.status.idle":"2025-12-04T21:34:12.045057Z","shell.execute_reply.started":"2025-12-04T21:32:28.550610Z","shell.execute_reply":"2025-12-04T21:34:12.044305Z"}},"outputs":[{"name":"stdout","text":"â³ Loading and Preprocessing...\nâœ… Data Ready: (35638, 4) | Users: 1000\nğŸš€ Calculating Discounted Similarities (Beta=1.0)...\n\nğŸ” Extracting top 199 neighbors (20%) per user...\n\nâœ… Results (Discounted Distance-Based Similarity):\nUser 7799420340:\n   -> Top Neighbors: ['B01929H4VM', 'B001GFKIBE', 'B00330UFQS', 'B00124BPBG', 'B00BWGHIHY']...\n   -> Scores: [0.571 0.562 0.5   0.5   0.5  ]\nUser 9434682614:\n   -> Top Neighbors: ['B009VLX8FS', 'B0092MKTL2', 'B008GVTSL2', 'B0092MKTWQ', 'B005MVLI8A']...\n   -> Scores: [0.4  0.25 0.25 0.25 0.25]\nUser 9714721180:\n   -> Top Neighbors: ['B005MVLI8A', 'B015U2LLIW', 'B00136NUG6', 'B01929H4VM', 'B001GFKIBE']...\n   -> Scores: [0.732 0.65  0.5   0.417 0.4  ]\nUser B000JF21D0:\n   -> Top Neighbors: ['B00TJ6S4RW', 'B0043ZDFEQ', 'B000W11N6W', 'B001GH1ZQE', 'B004EI3ON4']...\n   -> Scores: [0.417 0.417 0.417 0.4   0.4  ]\nUser B000QOEN6W:\n   -> Top Neighbors: ['B000VZV9KY', 'B00137QS28', 'B000VWMTHE', 'B00136NFOI', 'B00GLP4DMO']...\n   -> Scores: [0.5 0.5 0.5 0.5 0.4]\n\nAverage neighbors found per user: 146.2\nTotal Users processed: 1000\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# ---------------------------------------------------------\n# 1. Setup Data for Prediction\n# ---------------------------------------------------------\n# Calculate User Means (needed for the formula: r_v - mean_v)\n# We treat 0 as missing, so we replace 0 with NaN to get the true mean\nuser_means = user_item_matrix.replace(0, np.nan).mean(axis=1)\n\n# Create a \"Centered\" matrix (Rating - User Mean)\n# This allows us to quickly grab (r_{v,i} - mean_v) during the loop\nratings_diff = user_item_matrix.replace(0, np.nan).sub(user_means, axis=0)\n\n# ---------------------------------------------------------\n# 2. Prediction Function\n# ---------------------------------------------------------\ndef predict_second_round(target_users, neighbors_dict, similarity_df, ratings_diff_matrix, user_means):\n    predictions = []\n    \n    print(f\"ğŸš€ Starting Prediction for {len(target_users)} users using Discounted Similarities...\")\n    \n    for i, user in enumerate(target_users):\n        # Progress indicator\n        if i % 100 == 0 and i > 0:\n            print(f\"   ...Processed {i} users\")\n            \n        # 1. Get the specific Top 20% neighbors for this user\n        neighbors = neighbors_dict.get(user, [])\n        \n        if not neighbors:\n            continue\n            \n        # 2. Identify items to predict\n        # We only predict items that the neighbors have rated but the user hasn't\n        # Getting neighbors' ratings (centered)\n        neighbor_ratings = ratings_diff_matrix.loc[neighbors]\n        \n        # Identify items where at least one neighbor has a rating (not NaN)\n        # and the target user has NOT rated (value is NaN in ratings_diff_matrix or 0 in original)\n        candidates = neighbor_ratings.count() > 0 # Items neighbors rated\n        user_rated = ~ratings_diff_matrix.loc[user].isna() # Items user rated\n        items_to_predict = candidates.index[candidates & ~user_rated]\n        \n        # If too many items, let's limit to top 50 most popular among neighbors to save time\n        if len(items_to_predict) > 50:\n            items_to_predict = items_to_predict[:50]\n            \n        # 3. Prediction Loop per Item\n        for item in items_to_predict:\n            # Get neighbors who actually rated this specific item\n            # (Slice the centered ratings for these neighbors on this item)\n            item_ratings = neighbor_ratings[item].dropna()\n            valid_raters = item_ratings.index\n            \n            if len(valid_raters) == 0:\n                continue\n                \n            # Get Similarities (DS scores) for these specific raters\n            # These are the \"Discounted\" weights\n            sim_scores = similarity_df.loc[user, valid_raters]\n            \n            # --- THE FORMULA ---\n            # Numerator: Sum( Sim * (NeighborRating - NeighborMean) )\n            numerator = np.dot(sim_scores, item_ratings)\n            \n            # Denominator: Sum( |Sim| )\n            denominator = sim_scores.abs().sum()\n            \n            if denominator == 0:\n                continue\n                \n            # Calculate offset\n            pred_offset = numerator / denominator\n            \n            # Final Prediction = UserMean + Offset\n            final_pred = user_means[user] + pred_offset\n            \n            # Clip between 1 and 5\n            final_pred = min(5.0, max(1.0, final_pred))\n            \n            predictions.append({\n                'User': user,\n                'Item': item,\n                'Predicted_Rating': round(final_pred, 2),\n                'Contributing_Neighbors': len(valid_raters)\n            })\n            \n    return pd.DataFrame(predictions)\n\n# ---------------------------------------------------------\n# 3. Run the Prediction\n# ---------------------------------------------------------\n# We will predict for all users in our filtered set\nall_users = user_item_matrix.index\n\n# Run!\npred_df = predict_second_round(\n    target_users=all_users,\n    neighbors_dict=top_20_neighbors, # Dictionary from previous step\n    similarity_df=sim_df,            # Matrix from previous step\n    ratings_diff_matrix=ratings_diff,\n    user_means=user_means\n)\n\n# ---------------------------------------------------------\n# 4. Display & Analyze Results\n# ---------------------------------------------------------\nif not pred_df.empty:\n    print(\"\\nâœ… Prediction Complete!\")\n    print(f\"Generated {len(pred_df)} predictions.\")\n    \n    # Sort by highest predicted rating to see recommendations\n    top_recs = pred_df.sort_values(by=['User', 'Predicted_Rating'], ascending=[True, False])\n    \n    print(\"\\nğŸ”¹ Sample Recommendations (User-Based CF with Discounted Similarity):\")\n    print(top_recs.head(10))\n    \n    # Optional: Save to CSV\n    # top_recs.to_csv(\"Discounted_Predictions.csv\", index=False)\nelse:\n    print(\"âŒ No predictions generated. Check if neighbors overlap on unrated items.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T21:35:32.406981Z","iopub.execute_input":"2025-12-04T21:35:32.407857Z","iopub.status.idle":"2025-12-04T21:36:05.938864Z","shell.execute_reply.started":"2025-12-04T21:35:32.407831Z","shell.execute_reply":"2025-12-04T21:36:05.938213Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Starting Prediction for 1000 users using Discounted Similarities...\n   ...Processed 100 users\n   ...Processed 200 users\n   ...Processed 300 users\n   ...Processed 400 users\n   ...Processed 500 users\n   ...Processed 600 users\n   ...Processed 700 users\n   ...Processed 800 users\n   ...Processed 900 users\n\nâœ… Prediction Complete!\nGenerated 49983 predictions.\n\nğŸ”¹ Sample Recommendations (User-Based CF with Discounted Similarity):\n          User                  Item  Predicted_Rating  Contributing_Neighbors\n0   7799420340  A0072041HVZ3465DXUOR              5.00                       6\n2   7799420340        A101PYPZ8ASKCV              5.00                       6\n5   7799420340        A1063L1ALPA9WZ              5.00                       1\n7   7799420340        A107OPFJV11RKO              5.00                       2\n9   7799420340        A10D50UBEF3NUF              5.00                       1\n10  7799420340        A10DYTGIWV3RJZ              5.00                       3\n11  7799420340        A10FRUJV17R7TG              5.00                       3\n13  7799420340        A10KHX41ONY4U1              5.00                       5\n14  7799420340        A10LIO72DWUSOG              5.00                       2\n16  7799420340        A10NPPQEOF8QN6              5.00                       3\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ---------------------------------------------------------\n# 1. Setup Data (Assume 'user_item_matrix' exists from previous steps)\n# ---------------------------------------------------------\n# Ensure we are using the exact same matrix for fair comparison\n# If user_item_matrix is not in memory, please run the loading block from previous steps.\n\n# ---------------------------------------------------------\n# 2. Generate List A: Standard Pearson/Cosine (Step 2 Approach)\n# ---------------------------------------------------------\nprint(\"ğŸ”„ Generating List A (Standard Cosine)...\")\n# Mean centering\nuser_means = user_item_matrix.replace(0, np.nan).mean(axis=1)\nmatrix_centered = user_item_matrix.sub(user_means, axis=0).fillna(0)\n\n# Standard Cosine\nsim_matrix_standard = cosine_similarity(matrix_centered)\nsim_df_step2 = pd.DataFrame(sim_matrix_standard, index=user_item_matrix.index, columns=user_item_matrix.index)\n\n# Get Top 20% for Step 2\nneighbors_step2 = {}\nk_percent = 0.20\nfor user in sim_df_step2.index:\n    # Sort descending, drop self\n    candidates = sim_df_step2.loc[user].drop(user).sort_values(ascending=False)\n    k = max(1, int(len(candidates) * k_percent))\n    neighbors_step2[user] = candidates.head(k).index.tolist()\n\n# ---------------------------------------------------------\n# 3. Generate List B: Discounted DS (Step 5 Approach)\n# ---------------------------------------------------------\n# Assuming 'sim_df' from the previous Discounted calculation is available.\n# If not, we use the logic from the previous prompt (sim_df in code block above).\n# For this comparison script, let's assume `top_20_neighbors` (from Step 5) is ready.\nneighbors_step5 = top_20_neighbors # From previous code block\n\n# ---------------------------------------------------------\n# 4. Compare the Lists\n# ---------------------------------------------------------\ncomparison_results = []\n\nprint(\"\\nâš”ï¸  Comparing Neighborhoods (Step 2 vs Step 5)...\\n\")\n\nusers_to_check = list(neighbors_step5.keys())[:10] # Check first 10 users\n\nfor user in users_to_check:\n    list_a = set(neighbors_step2.get(user, []))\n    list_b = set(neighbors_step5.get(user, []))\n    \n    # Calculate Jaccard Similarity (Intersection over Union)\n    intersection = list_a.intersection(list_b)\n    union = list_a.union(list_b)\n    \n    jaccard_score = len(intersection) / len(union) if len(union) > 0 else 0\n    \n    comparison_results.append({\n        'User': user,\n        'Size_Step2': len(list_a),\n        'Size_Step5': len(list_b),\n        'Overlap_Count': len(intersection),\n        'Jaccard_Sim': round(jaccard_score, 3)\n    })\n\n# Create DataFrame\ncomp_df = pd.DataFrame(comparison_results)\n\nprint(comp_df)\n\nprint(\"\\n-------------------------------------------------------------\")\nprint(f\"Average Jaccard Overlap: {comp_df['Jaccard_Sim'].mean():.3f}\")\nprint(\"-------------------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T21:37:30.395084Z","iopub.execute_input":"2025-12-04T21:37:30.395402Z","iopub.status.idle":"2025-12-04T21:37:31.251911Z","shell.execute_reply.started":"2025-12-04T21:37:30.395376Z","shell.execute_reply":"2025-12-04T21:37:31.251197Z"}},"outputs":[{"name":"stdout","text":"ğŸ”„ Generating List A (Standard Cosine)...\n\nâš”ï¸  Comparing Neighborhoods (Step 2 vs Step 5)...\n\n         User  Size_Step2  Size_Step5  Overlap_Count  Jaccard_Sim\n0  7799420340         199         199            162         0.69\n1  9434682614         199          13              9         0.04\n2  9714721180         199          73             53         0.24\n3  B000JF21D0         199          73             57         0.27\n4  B000QOEN6W         199         199            169         0.74\n5  B000QP4IBG         199         132            115         0.53\n6  B000S3BNY6         199         199            173         0.77\n7  B000S51XMQ         199          93             61         0.26\n8  B000S56380         199         199            179         0.82\n9  B000SXM2AK         199         199            155         0.64\n\n-------------------------------------------------------------\nAverage Jaccard Overlap: 0.499\n-------------------------------------------------------------\n","output_type":"stream"}],"execution_count":69}]}